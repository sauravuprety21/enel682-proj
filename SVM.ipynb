{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the needed libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler, Normalizer\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the training CV data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the processed IMU data \n",
    "IMU_data = pd.read_csv('data/training.csv')\n",
    "\n",
    "# Previewing the IMU data\n",
    "IMU_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = 'axdt_var,aydt_var,gzdt_var'.split(',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separating the features (IMU data)\n",
    "X = IMU_data[[col for col in IMU_data.columns if col !='motion_state']].copy()\n",
    "X = IMU_data[feature_cols]\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = IMU_data['motion_state'].copy()\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data into testing and training\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the Pipeline and Parameter Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up pipeline to test different scaling techniques with SVM\n",
    "pipe = Pipeline([('preprocessor', StandardScaler()),\n",
    "                 ('classifier', [SVC()])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the parameter grid to tune the hyperparameters and get the best results\n",
    "param_grid = [{'preprocessor': [StandardScaler(), RobustScaler()],\n",
    "               'classifier': [SVC()],\n",
    "               'classifier__C': [0.001, 0.01, 0.1, 1 ,10],\n",
    "               'classifier__gamma': [0.001, 0.01, 0.1,1],\n",
    "               'classifier__kernel': ['rbf','poly']\n",
    "              }]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing a grid search across all the set hyperparameters\n",
    "grid = GridSearchCV(pipe, param_grid, cv=5, scoring='accuracy', return_train_score=True)\n",
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search Best Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best parameters\n",
    "print(\"Best Parameters:\")\n",
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best cross-validation train score\n",
    "print(\"Best CV Train Score:\")\n",
    "grid.cv_results_['mean_train_score'][grid.best_index_]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance on Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test set accuracy\n",
    "best_test_model = grid.best_estimator_\n",
    "print(\"Test-set score:\")\n",
    "best_test_model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing on completely new dataset\n",
    "IMU_data2 = pd.read_csv('data/testing.csv')\n",
    "X2 = IMU_data2[[col for col in IMU_data.columns if col !='motion_state']].copy()\n",
    "X2 = IMU_data2[feature_cols]\n",
    "y2 = IMU_data2['motion_state'].copy()\n",
    "print(\"Testing 2 set score:\")\n",
    "best_test_model.score(X2, y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing on completely new dataset\n",
    "IMU_data3 = pd.read_csv('data/testing2.csv')\n",
    "X3 = IMU_data3[[col for col in IMU_data.columns if col !='motion_state']].copy()\n",
    "X3 = IMU_data3[feature_cols]\n",
    "y3 = IMU_data3['motion_state'].copy()\n",
    "print(\"Testing 3 set score:\")\n",
    "best_test_model.score(X3, y3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for outputting the confusion matrix\n",
    "def evaluate_classification(true_values, predicted_values, ticklabels, model):\n",
    "    # Calculate confusion matrix\n",
    "    confusion_mat = confusion_matrix(true_values, predicted_values)\n",
    "    \n",
    "    # Plot heatmap\n",
    "    sns.heatmap(confusion_mat, xticklabels=ticklabels, yticklabels=ticklabels, annot=True, fmt='d', cbar=False, cmap='viridis')\n",
    "    plt.xlabel('Predicted values')\n",
    "    plt.ylabel('True values')\n",
    "    plt.title('Confusion Matrix - ' + model)\n",
    "    plt.show()\n",
    "\n",
    "    # Calculate scores\n",
    "    accuracy = accuracy_score(true_values, predicted_values)\n",
    "    precision = precision_score(true_values, predicted_values, average='macro')\n",
    "    recall = recall_score(true_values, predicted_values, average='macro')\n",
    "    f1 = f1_score(true_values, predicted_values, average='macro')\n",
    "\n",
    "    '''# Calculate additional metrics\n",
    "    tn, fp, fn, tp = confusion_mat.ravel()\n",
    "\n",
    "    # Print additional metrics\n",
    "    print(\"Metrics for\", model)\n",
    "    print(\"True Negatives:\", tn)\n",
    "    print(\"False Positives:\", fp)\n",
    "    print(\"False Negatives:\", fn)\n",
    "    print(\"True Positives:\", tp)'''\n",
    "\n",
    "    # Print classification report\n",
    "    print(\"\\nClassification Report for\", model + \":\")\n",
    "    print(classification_report(true_values, predicted_values))\n",
    "\n",
    "    # Print scores\n",
    "    print(\"\\nAccuracy Score:\", accuracy)\n",
    "    print(\"Precision Score:\", precision)\n",
    "    print(\"Recall Score:\", recall)\n",
    "    print(\"F1 Score:\", f1)\n",
    "    \n",
    "pred_val = grid.predict(X_test)\n",
    "evaluate_classification(y_test, pred_val, ['rest', 'straight', 'turn'], 'Support Vector Classifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_val = grid.predict(X2)\n",
    "evaluate_classification(y2, pred_val, ['rest', 'straight', 'turn'], 'Support Vector Classifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_val = grid.predict(X3)\n",
    "evaluate_classification(y3, pred_val, ['rest', 'straight', 'turn'], 'Support Vector Classifier')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "proj682_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
